{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f",
   "metadata": {},
   "source": [
    "# [모의 캐글-의료] 흉부 CT 코로나 감염 여부 분류\n",
    "- 이미지 binary 분류 과제\n",
    "- 담당: 이녕민M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4325d39-6344-4116-b343-df51696905ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!apt-get update && apt-get install -y python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f475804-13db-484c-a348-f01580e80a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#MBConvBlock\n",
    "import math\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#ScaledDotScaling\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e500fc34-eed3-4fe6-af43-56ef3dcfaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b84fa5a-81d6-4568-8180-7d10ea3b463a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:th637tlo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 102301... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">radiant-etchings-51</strong>: <a href=\"https://wandb.ai/jch/my-test-project/runs/th637tlo\" target=\"_blank\">https://wandb.ai/jch/my-test-project/runs/th637tlo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220214_073310-th637tlo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:th637tlo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jch/my-test-project/runs/2ybc2e7z\" target=\"_blank\">angelic-violet-52</a></strong> to <a href=\"https://wandb.ai/jch/my-test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/jch/my-test-project/runs/2ybc2e7z?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc68c3baa60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"my-test-project\", entity=\"jch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c478911d-6ae7-4fa7-8b33-5744e4d6a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.0005,\n",
    "  \"epochs\": 50,\n",
    "  \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Arguments & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9c63836-0a7a-48f7-a5ea-e16b617fec65",
   "metadata": {},
   "source": [
    "# 데이터 디렉토리 구조\n",
    "\n",
    "data/  \n",
    "  \\_train/  \n",
    "    \\_0.png  \n",
    "    \\_1.png  \n",
    "    \\_...  \n",
    "  \\_test/  \n",
    "    \\_0.png  \n",
    "    \\_1.png  \n",
    "    \\_...  \n",
    "  \\_train.csv  \n",
    "  \\_sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "### 데이터 디렉토리 설정 ###\n",
    "DATA_DIR= 'data'\n",
    "NUM_CLS = 2\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32 #32\n",
    "LEARNING_RATE = 0.0005 #0.0005\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "INPUT_SHAPE = 128 # 128\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {},
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Dataset split\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "        return trans_image, data['COVID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f42d51-3862-431a-9d5d-648353740954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rwightman/pytorch-image-models.git\n",
      "  Cloning https://github.com/rwightman/pytorch-image-models.git to /tmp/pip-req-build-bg4qy3_t\n",
      "Requirement already satisfied (use --upgrade to upgrade): timm==0.5.5 from git+https://github.com/rwightman/pytorch-image-models.git in /opt/conda/lib/python3.8/site-packages\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm==0.5.5) (1.7.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm==0.5.5) (0.8.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm==0.5.5) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm==0.5.5) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm==0.5.5) (8.1.0)\n",
      "Building wheels for collected packages: timm\n",
      "  Building wheel for timm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for timm: filename=timm-0.5.5-py3-none-any.whl size=432802 sha256=e7f62f3a8537bf893e634e012cdc3515def15a130fad23edbb0432a294eaa4c1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lf7vjwez/wheels/b3/03/6a/79956ddc149294ccf13727ca946e8baf38ccbe593299074e86\n",
      "Successfully built timm\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rwightman/pytorch-image-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "671c3ecc-938b-4f70-bbe0-a10c96f250b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from functools import partial\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# class SwishImplementation(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, i):\n",
    "#         result = i * torch.sigmoid(i)\n",
    "#         ctx.save_for_backward(i)\n",
    "#         return result\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         i = ctx.saved_variables[0]\n",
    "#         sigmoid_i = torch.sigmoid(i)\n",
    "#         return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "# class MemoryEfficientSwish(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return SwishImplementation.apply(x)\n",
    "\n",
    "\n",
    "# def drop_connect(inputs, p, training):\n",
    "#     \"\"\" Drop connect. \"\"\"\n",
    "#     if not training: return inputs\n",
    "#     batch_size = inputs.shape[0]\n",
    "#     keep_prob = 1 - p\n",
    "#     random_tensor = keep_prob\n",
    "#     random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "#     binary_tensor = torch.floor(random_tensor)\n",
    "#     output = inputs / keep_prob * binary_tensor\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def get_same_padding_conv2d(image_size=None):\n",
    "#      return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "\n",
    "# def get_width_and_height_from_size(x):\n",
    "#     \"\"\" Obtains width and height from a int or tuple \"\"\"\n",
    "#     if isinstance(x, int): return x, x\n",
    "#     if isinstance(x, list) or isinstance(x, tuple): return x\n",
    "#     else: raise TypeError()\n",
    "\n",
    "# def calculate_output_image_size(input_image_size, stride):\n",
    "#     \"\"\"\n",
    "#     计算出 Conv2dSamePadding with a stride.\n",
    "#     \"\"\"\n",
    "#     if input_image_size is None: return None\n",
    "#     image_height, image_width = get_width_and_height_from_size(input_image_size)\n",
    "#     stride = stride if isinstance(stride, int) else stride[0]\n",
    "#     image_height = int(math.ceil(image_height / stride))\n",
    "#     image_width = int(math.ceil(image_width / stride))\n",
    "#     return [image_height, image_width]\n",
    "\n",
    "\n",
    "\n",
    "# class Conv2dStaticSamePadding(nn.Conv2d):\n",
    "#     \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
    "#         super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "#         self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "#         # Calculate padding based on image size and save it\n",
    "#         assert image_size is not None\n",
    "#         ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size\n",
    "#         kh, kw = self.weight.size()[-2:]\n",
    "#         sh, sw = self.stride\n",
    "#         oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "#         pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "#         pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "#         if pad_h > 0 or pad_w > 0:\n",
    "#             self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "#         else:\n",
    "#             self.static_padding = Identity()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.static_padding(x)\n",
    "#         x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "#         return x\n",
    "\n",
    "# class Identity(nn.Module):\n",
    "#     def __init__(self, ):\n",
    "#         super(Identity, self).__init__()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return input\n",
    "\n",
    "\n",
    "# # MBConvBlock\n",
    "# class MBConvBlock(nn.Module):\n",
    "#     '''\n",
    "#     层 ksize3*3 输入32 输出16  conv1  stride步长1\n",
    "#     '''\n",
    "#     def __init__(self, ksize, input_filters, output_filters, expand_ratio=1, stride=1, image_size=224):\n",
    "#         super().__init__()\n",
    "#         self._bn_mom = 0.1\n",
    "#         self._bn_eps = 0.01\n",
    "#         self._se_ratio = 0.25\n",
    "#         self._input_filters = input_filters\n",
    "#         self._output_filters = output_filters\n",
    "#         self._expand_ratio = expand_ratio\n",
    "#         self._kernel_size = ksize\n",
    "#         self._stride = stride\n",
    "\n",
    "#         inp = self._input_filters\n",
    "#         oup = self._input_filters * self._expand_ratio\n",
    "#         if self._expand_ratio != 1:\n",
    "#             Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "#             self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "#             self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "\n",
    "#         # Depthwise convolution\n",
    "#         k = self._kernel_size\n",
    "#         s = self._stride\n",
    "#         Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "#         self._depthwise_conv = Conv2d(\n",
    "#             in_channels=oup, out_channels=oup, groups=oup,\n",
    "#             kernel_size=k, stride=s, bias=False)\n",
    "#         self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "#         image_size = calculate_output_image_size(image_size, s)\n",
    "\n",
    "#         # Squeeze and Excitation layer, if desired\n",
    "#         Conv2d = get_same_padding_conv2d(image_size=(1,1))\n",
    "#         num_squeezed_channels = max(1, int(self._input_filters * self._se_ratio))\n",
    "#         self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "#         self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "#         # Output phase\n",
    "#         final_oup = self._output_filters\n",
    "#         Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "#         self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "#         self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "#         self._swish = MemoryEfficientSwish()\n",
    "\n",
    "#     def forward(self, inputs, drop_connect_rate=None):\n",
    "#         \"\"\"\n",
    "#         :param inputs: input tensor\n",
    "#         :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "#         :return: output of block\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Expansion and Depthwise Convolution\n",
    "#         x = inputs\n",
    "#         if self._expand_ratio != 1:\n",
    "#             expand = self._expand_conv(inputs)\n",
    "#             bn0 = self._bn0(expand)\n",
    "#             x = self._swish(bn0)\n",
    "#         depthwise = self._depthwise_conv(x)\n",
    "#         bn1 = self._bn1(depthwise)\n",
    "#         x = self._swish(bn1)\n",
    "\n",
    "#         # Squeeze and Excitation\n",
    "#         x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "#         x_squeezed = self._se_reduce(x_squeezed)\n",
    "#         x_squeezed = self._swish(x_squeezed)\n",
    "#         x_squeezed = self._se_expand(x_squeezed)\n",
    "#         x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "#         x = self._bn2(self._project_conv(x))\n",
    "\n",
    "#         # Skip connection and drop connect\n",
    "#         input_filters, output_filters = self._input_filters, self._output_filters\n",
    "#         if self._stride == 1 and input_filters == output_filters:\n",
    "#             if drop_connect_rate:\n",
    "#                 x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "#             x = x + inputs  # skip connection\n",
    "#         return x\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input=torch.randn(1,3,112,112)\n",
    "#     mbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=3,image_size=112)\n",
    "#     out=mbconv(input)\n",
    "#     print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1471fca-f735-4af0-b7fd-983d49ec833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import init\n",
    "\n",
    "\n",
    "\n",
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     '''\n",
    "#     Scaled dot-product attention\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, d_model, d_k, d_v, h,dropout=.1):\n",
    "#         '''\n",
    "#         :param d_model: Output dimensionality of the model\n",
    "#         :param d_k: Dimensionality of queries and keys\n",
    "#         :param d_v: Dimensionality of values\n",
    "#         :param h: Number of heads\n",
    "#         '''\n",
    "#         super(ScaledDotProductAttention, self).__init__()\n",
    "#         self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "#         self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "#         self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "#         self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "#         self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "#         self.d_model = d_model\n",
    "#         self.d_k = d_k\n",
    "#         self.d_v = d_v\n",
    "#         self.h = h\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "#                 if m.bias is not None:\n",
    "#                     init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 init.constant_(m.weight, 1)\n",
    "#                 init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 init.normal_(m.weight, std=0.001)\n",
    "#                 if m.bias is not None:\n",
    "#                     init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "#         '''\n",
    "#         Computes\n",
    "#         :param queries: Queries (b_s, nq, d_model)\n",
    "#         :param keys: Keys (b_s, nk, d_model)\n",
    "#         :param values: Values (b_s, nk, d_model)\n",
    "#         :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n",
    "#         :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n",
    "#         :return:\n",
    "#         '''\n",
    "#         b_s, nq = queries.shape[:2]\n",
    "#         nk = keys.shape[1]\n",
    "\n",
    "#         q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "#         k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "#         v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "#         att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "#         if attention_weights is not None:\n",
    "#             att = att * attention_weights\n",
    "#         if attention_mask is not None:\n",
    "#             att = att.masked_fill(attention_mask, -np.inf)\n",
    "#         att = torch.softmax(att, -1)\n",
    "#         att=self.dropout(att)\n",
    "\n",
    "#         out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "#         out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input=torch.randn(50,49,512)\n",
    "#     sa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\n",
    "#     output=sa(input,input,input)\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c69601-4fb6-4ac8-bb3a-ced564d46ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn, sqrt\n",
    "# import torch\n",
    "# import sys\n",
    "# from math import sqrt\n",
    "# sys.path.append('.')\n",
    "# # from model.conv.MBConv import MBConvBlock\n",
    "# # from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "\n",
    "# class CoAtNet(nn.Module):\n",
    "#     def __init__(self,in_ch,image_size,out_chs=[8,16,25,40,64]): #[64,96,192,384,768]\n",
    "#         super().__init__()\n",
    "#         self.out_chs=out_chs\n",
    "#         self.maxpool2d=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "#         self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.s0=nn.Sequential(\n",
    "#             nn.Conv2d(in_ch,in_ch,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(in_ch,in_ch,kernel_size=3,padding=1)\n",
    "#         )\n",
    "#         self.mlp0=nn.Sequential(\n",
    "#             nn.Conv2d(in_ch,out_chs[0],kernel_size=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(out_chs[0],out_chs[0],kernel_size=1)\n",
    "#         )\n",
    "        \n",
    "#         self.s1=MBConvBlock(ksize=3,input_filters=out_chs[0],output_filters=out_chs[0],image_size=image_size//2)\n",
    "#         self.mlp1=nn.Sequential(\n",
    "#             nn.Conv2d(out_chs[0],out_chs[1],kernel_size=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(out_chs[1],out_chs[1],kernel_size=1)\n",
    "#         )\n",
    "\n",
    "#         # self.s2=MBConvBlock(ksize=3,input_filters=out_chs[1],output_filters=out_chs[1],image_size=image_size//4)\n",
    "#         # self.mlp2=nn.Sequential(\n",
    "#         #     nn.Conv2d(out_chs[1],out_chs[2],kernel_size=1),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.Conv2d(out_chs[2],out_chs[2],kernel_size=1)\n",
    "#         # )\n",
    "\n",
    "#         self.s3=ScaledDotProductAttention(out_chs[1],out_chs[1]//8,out_chs[1]//8,8)\n",
    "#         self.mlp3=nn.Sequential(\n",
    "#             nn.Linear(out_chs[1],out_chs[2]),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(out_chs[2],out_chs[2])\n",
    "#         )\n",
    "\n",
    "#         # self.s4=ScaledDotProductAttention(out_chs[3],out_chs[3]//8,out_chs[3]//8,8)\n",
    "#         # self.mlp4=nn.Sequential(\n",
    "#         #     nn.Linear(out_chs[2],out_chs[3]),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.Linear(out_chs[3],out_chs[3])\n",
    "#         # )\n",
    "        \n",
    "#         self.Final_layer=nn.Sequential(\n",
    "#             # nn.Linear(24576, 24576//8),\n",
    "#             # nn.ReLU(),\n",
    "#             # nn.Linear(24576//8 , 3072//8),\n",
    "#             # nn.ReLU(),\n",
    "#             # nn.Linear(3072//8 , 384//8),\n",
    "#             # nn.ReLU(),\n",
    "#             # nn.Linear(384//8 , 2),\n",
    "#             nn.Linear(12800, 12800//8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(12800//8, 1600//8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1600//8, 200//8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(200//8, 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x) :\n",
    "#         B,C,H,W=x.shape\n",
    "#         #stage0\n",
    "#         y=self.mlp0(self.s0(x))\n",
    "#         y=self.maxpool2d(y)\n",
    "#         #stage1\n",
    "#         y=self.mlp1(self.s1(y))\n",
    "#         y=self.maxpool2d(y)\n",
    "#         #stage2\n",
    "#         # y=self.mlp2(self.s2(y))\n",
    "#         # y=self.maxpool2d(y)\n",
    "#         #stage3\n",
    "#         # print('###1',y.shape) #[8,64,32,32]\n",
    "#         y=y.reshape(B,self.out_chs[1],-1).permute(0,2,1) #B,N,C\n",
    "#         y=self.mlp3(self.s3(y,y,y))\n",
    "#         y=self.maxpool1d(y.permute(0,2,1)).permute(0,2,1)\n",
    "#         # print('###2',y.shape)\n",
    "#         #stage4\n",
    "#         # y=self.mlp4(self.s4(y,y,y))\n",
    "#         # y=self.maxpool1d(y.permute(0,2,1))\n",
    "#         # N=y.shape[-1]\n",
    "#         # print('###3',y.shape)\n",
    "#         # y=y.reshape(B,self.out_chs[2],int(sqrt(N)),int(sqrt(N)))\n",
    "        \n",
    "#         # print(y.shape)\n",
    "#         y=torch.flatten(y,1)\n",
    "        \n",
    "\n",
    "#         output=self.Final_layer(y)\n",
    "#         return output\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# #     x=torch.randn(5,3,128,128)\n",
    "# #     coatnet=CoAtNet(3,128)\n",
    "# #     print(coatnet)\n",
    "# #     y=coatnet(x)\n",
    "# #     print(y)\n",
    "# #     print(y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5a587b-054c-4e7e-8149-e76ad94638cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "\n",
    "# def COVID19Model(pretrained=True):\n",
    "#     model = timm.create_model('swin_base_patch4_window12_384', pretrained=pretrained)\n",
    "#     n_features = model.head.in_features  \n",
    "#     model.classifier = nn.Sequential(nn.Linear(n_features, 2) , nn.Softmax(dim=1))\n",
    "#     model = model.to(DEVICE) \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "685e0b73-f323-40ea-b372-6c1d607618a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4564, 0.5436],\n",
      "        [0.4397, 0.5603],\n",
      "        [0.4968, 0.5032],\n",
      "        [0.4997, 0.5003],\n",
      "        [0.5366, 0.4634]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class custom_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(custom_CNN, self).__init__()\n",
    "        self.layer1=nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                        nn.BatchNorm2d(32)\n",
    "                    )\n",
    "        \n",
    "        self.layer2=nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                        nn.BatchNorm2d(32)\n",
    "                    )\n",
    "        \n",
    "        self.layer3=nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                        nn.BatchNorm2d(64)\n",
    "                    )\n",
    "        \n",
    "        self.layer4=nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                        nn.BatchNorm2d(64)\n",
    "                    )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Linear(in_features=64*14*14, out_features=64*14),\n",
    "                        nn.Dropout(p=0.3)\n",
    "                    )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                        nn.Linear(in_features=64*14, out_features=128),\n",
    "                        nn.Dropout(p=0.3)\n",
    "                    )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                        nn.Linear(in_features=128, out_features=num_classes),\n",
    "                        nn.Dropout(p=0.3)\n",
    "                    )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        output = self.softmax(x)\n",
    "        # print(output)\n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x=torch.randn(5,3,128,128)\n",
    "    model=custom_CNN(2)\n",
    "\n",
    "    y=model(x)\n",
    "    print(y)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056905-1f77-4579-a260-07bb1056f6db",
   "metadata": {},
   "source": [
    "## Utils\n",
    "### EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가, 감소 시 0으로 리셋\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "        self.save_model = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            return None\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "        \n",
    "        # print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" epoch에 대한 학습 및 검증 절차 정의\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, model, device, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "            wandb.log({\"train_loss\": loss})\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, F1-Macro: {f1}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "            wandb.log({\"valid_loss\": loss})\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, F1-Macro: {f1}'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    \n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    f1 = f1_score(y_answer, y_pred, average='macro')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train\n",
    "### 학습을 위한 객체 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19610a4-ad7c-44a0-80cd-9734b5015100",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n",
      "Train set samples: 581 Val set samples: 65\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))\n",
    "# print(train_dataset[1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8dae0-8e32-4ac0-a585-858a7095d2a4",
   "metadata": {},
   "source": [
    "#### Load model and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c398d76-a0c4-460c-a9a4-4f141ce41d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4d52e1-752a-40d5-9b34-c06d3dbdd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = custom_CNN(NUM_CLS).to(DEVICE)\n",
    "# model = CoAtNet(3, 128).to(DEVICE)\n",
    "# model = COVID19Model(pretrained=False)\n",
    "\n",
    "# # Save Initial Model\n",
    "# torch.save(model.state_dict(), 'initial.pt')\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(loss_fn, model, DEVICE, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b881024-3921-4c2c-b9ec-e6b23c4f5ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=12544, out_features=896, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=896, out_features=128, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa8aef-b984-4133-b6b2-e1c85900f724",
   "metadata": {},
   "source": [
    "### epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcc35f70-25fc-48e1-92f8-8633b3b8be80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.7290441791216532, Acc: 0.5559380378657487, F1-Macro: 0.4672982885085575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 1/50 [01:00<49:21, 60.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val loss: 1.04808709025383, Acc: 0.49230769230769234, F1-Macro: 0.32989690721649484\n",
      "Epoch 1, Train loss: 0.6799141532844968, Acc: 0.6333907056798623, F1-Macro: 0.6056875209100033\n",
      "Epoch 1, Val loss: 1.028282105922699, Acc: 0.5538461538461539, F1-Macro: 0.48342011510002736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4% 2/50 [02:16<52:03, 65.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.5576693101061715, Acc: 0.7142857142857143, F1-Macro: 0.7071003401360545\n",
      "Epoch 2, Val loss: 1.1849801242351532, Acc: 0.6, F1-Macro: 0.5921814671814671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6% 3/50 [03:17<50:02, 63.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.5092956539657381, Acc: 0.7349397590361446, F1-Macro: 0.7279697913119618\n",
      "Epoch 3, Val loss: 0.8630536794662476, Acc: 0.676923076923077, F1-Macro: 0.6766169154228856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% 4/50 [04:19<48:32, 63.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.37070184614923263, Acc: 0.8123924268502581, F1-Macro: 0.8089490371881005\n",
      "Epoch 4, Val loss: 0.6987018138170242, Acc: 0.676923076923077, F1-Macro: 0.6741465743614228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 5/50 [05:19<46:42, 62.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 0.2975599898232354, Acc: 0.8416523235800344, F1-Macro: 0.8376700680272109\n",
      "Epoch 5, Val loss: 0.7066786475479603, Acc: 0.676923076923077, F1-Macro: 0.6756949394155382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12% 6/50 [06:18<44:54, 61.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train loss: 0.24651314235395855, Acc: 0.8795180722891566, F1-Macro: 0.8776915674245158\n",
      "Epoch 6, Val loss: 0.5333724431693554, Acc: 0.7538461538461538, F1-Macro: 0.7523809523809524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14% 7/50 [07:20<44:11, 61.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train loss: 0.16429992201220658, Acc: 0.9122203098106713, F1-Macro: 0.9106921651069215\n",
      "Epoch 7, Val loss: 0.6634171605110168, Acc: 0.7076923076923077, F1-Macro: 0.7065811356616774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16% 8/50 [08:21<43:02, 61.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train loss: 0.1583604681202107, Acc: 0.9122203098106713, F1-Macro: 0.9104362571296439\n",
      "Epoch 8, Val loss: 1.0880047976970673, Acc: 0.7692307692307693, F1-Macro: 0.7683535281539557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18% 9/50 [09:20<41:28, 60.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train loss: 0.15812060381803247, Acc: 0.9156626506024096, F1-Macro: 0.9140327514411419\n",
      "Epoch 9, Val loss: 0.7184899002313614, Acc: 0.7538461538461538, F1-Macro: 0.7509578544061302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 10/50 [10:20<40:15, 60.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train loss: 0.16333163840075335, Acc: 0.891566265060241, F1-Macro: 0.8883908561183327\n",
      "Epoch 10, Val loss: 0.5822136886417866, Acc: 0.7076923076923077, F1-Macro: 0.7065811356616774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22% 11/50 [11:21<39:20, 60.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train loss: 0.1533229423997303, Acc: 0.9053356282271945, F1-Macro: 0.9029009684074908\n",
      "Epoch 11, Val loss: 1.2777624726295471, Acc: 0.7692307692307693, F1-Macro: 0.7683535281539557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24% 12/50 [12:20<38:10, 60.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train loss: 0.18204283507333863, Acc: 0.8640275387263339, F1-Macro: 0.8587796946005901\n",
      "Epoch 12, Val loss: 0.9115068316459656, Acc: 0.7230769230769231, F1-Macro: 0.7198275862068966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26% 13/50 [13:21<37:09, 60.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train loss: 0.17070158840053612, Acc: 0.8967297762478486, F1-Macro: 0.8937687413150004\n",
      "Epoch 13, Val loss: 0.543696629581973, Acc: 0.7538461538461538, F1-Macro: 0.7509578544061302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% 14/50 [14:20<36:03, 60.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train loss: 0.131683264962501, Acc: 0.9104991394148021, F1-Macro: 0.9084462653640089\n",
      "Epoch 14, Val loss: 0.7282745689153671, Acc: 0.7230769230769231, F1-Macro: 0.7214285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% 15/50 [15:20<34:56, 59.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train loss: 0.146047154131035, Acc: 0.9070567986230637, F1-Macro: 0.9046127839665321\n",
      "Epoch 15, Val loss: 0.5508373202756047, Acc: 0.7384615384615385, F1-Macro: 0.7374673319078165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32% 16/50 [16:20<33:57, 59.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train loss: 0.1479226876464155, Acc: 0.9018932874354562, F1-Macro: 0.8992562172028291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32% 16/50 [17:19<36:49, 64.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Val loss: 0.9211353063583374, Acc: 0.7538461538461538, F1-Macro: 0.7523809523809524\n",
      "Early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if early_stopper.save_model:\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, 'best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53514a-e83f-4795-9589-640f26cc2993",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference\n",
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = 'best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'sample_submission.csv'))\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'test',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efd72b-172d-4e34-a1dd-65ed8c745b58",
   "metadata": {},
   "source": [
    "### 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7230e-01, 2.7700e-02],\n",
      "        [8.4935e-01, 1.5065e-01],\n",
      "        [8.1651e-01, 1.8349e-01],\n",
      "        [3.4291e-01, 6.5709e-01],\n",
      "        [9.8791e-01, 1.2090e-02],\n",
      "        [7.4627e-03, 9.9254e-01],\n",
      "        [6.6846e-01, 3.3154e-01],\n",
      "        [7.5879e-02, 9.2412e-01],\n",
      "        [9.8790e-01, 1.2102e-02],\n",
      "        [1.8778e-05, 9.9998e-01],\n",
      "        [9.7802e-01, 2.1982e-02],\n",
      "        [9.3440e-01, 6.5602e-02],\n",
      "        [9.8545e-01, 1.4554e-02],\n",
      "        [9.5419e-01, 4.5813e-02],\n",
      "        [9.0247e-01, 9.7534e-02],\n",
      "        [9.8905e-01, 1.0951e-02],\n",
      "        [4.4628e-01, 5.5372e-01],\n",
      "        [9.9488e-01, 5.1226e-03],\n",
      "        [4.7339e-01, 5.2661e-01],\n",
      "        [9.8920e-01, 1.0798e-02],\n",
      "        [9.9853e-01, 1.4723e-03],\n",
      "        [9.9292e-01, 7.0771e-03],\n",
      "        [9.2529e-01, 7.4712e-02],\n",
      "        [9.6305e-01, 3.6955e-02],\n",
      "        [9.6643e-01, 3.3571e-02],\n",
      "        [9.9677e-01, 3.2302e-03],\n",
      "        [9.9283e-01, 7.1667e-03],\n",
      "        [3.0243e-01, 6.9757e-01],\n",
      "        [4.9650e-03, 9.9504e-01],\n",
      "        [1.0714e-02, 9.8929e-01],\n",
      "        [3.0673e-02, 9.6933e-01],\n",
      "        [1.7424e-03, 9.9826e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1690, 0.8310],\n",
      "        [0.9627, 0.0373],\n",
      "        [0.9866, 0.0134],\n",
      "        [0.2451, 0.7549],\n",
      "        [0.6401, 0.3599],\n",
      "        [0.0494, 0.9506],\n",
      "        [0.0623, 0.9377],\n",
      "        [0.0279, 0.9721],\n",
      "        [0.0047, 0.9953],\n",
      "        [0.5879, 0.4121],\n",
      "        [0.9903, 0.0097],\n",
      "        [0.0054, 0.9946],\n",
      "        [0.9679, 0.0321],\n",
      "        [0.0144, 0.9856],\n",
      "        [0.0157, 0.9843],\n",
      "        [0.9964, 0.0036],\n",
      "        [0.9759, 0.0241],\n",
      "        [0.8684, 0.1316],\n",
      "        [0.0302, 0.9698],\n",
      "        [0.0030, 0.9970],\n",
      "        [0.2906, 0.7094],\n",
      "        [0.9656, 0.0344],\n",
      "        [0.0278, 0.9722],\n",
      "        [0.7928, 0.2072],\n",
      "        [0.5523, 0.4477],\n",
      "        [0.0060, 0.9940],\n",
      "        [0.9895, 0.0105],\n",
      "        [0.7135, 0.2865],\n",
      "        [0.0092, 0.9908],\n",
      "        [0.9496, 0.0504],\n",
      "        [0.9962, 0.0038],\n",
      "        [0.9905, 0.0095]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7271e-03, 9.9027e-01],\n",
      "        [4.7026e-04, 9.9953e-01],\n",
      "        [9.1508e-01, 8.4919e-02],\n",
      "        [9.6868e-01, 3.1316e-02],\n",
      "        [9.2225e-03, 9.9078e-01],\n",
      "        [9.8523e-01, 1.4766e-02],\n",
      "        [9.9175e-01, 8.2498e-03],\n",
      "        [9.9114e-01, 8.8600e-03],\n",
      "        [9.7607e-01, 2.3931e-02],\n",
      "        [9.9221e-01, 7.7887e-03],\n",
      "        [6.3900e-02, 9.3610e-01],\n",
      "        [9.9521e-01, 4.7896e-03],\n",
      "        [9.9834e-01, 1.6568e-03],\n",
      "        [9.9659e-01, 3.4119e-03],\n",
      "        [4.6501e-01, 5.3499e-01],\n",
      "        [3.6431e-03, 9.9636e-01],\n",
      "        [9.3479e-01, 6.5208e-02],\n",
      "        [9.2841e-01, 7.1593e-02],\n",
      "        [9.6488e-01, 3.5123e-02],\n",
      "        [1.7765e-02, 9.8223e-01],\n",
      "        [3.5839e-02, 9.6416e-01],\n",
      "        [8.6134e-01, 1.3866e-01],\n",
      "        [6.9404e-01, 3.0596e-01],\n",
      "        [9.1473e-01, 8.5270e-02],\n",
      "        [9.8796e-01, 1.2040e-02],\n",
      "        [1.1598e-01, 8.8402e-01],\n",
      "        [9.9233e-01, 7.6669e-03],\n",
      "        [9.8980e-01, 1.0197e-02],\n",
      "        [3.0205e-01, 6.9795e-01],\n",
      "        [1.2204e-02, 9.8780e-01],\n",
      "        [9.9740e-01, 2.6034e-03],\n",
      "        [9.6445e-01, 3.5550e-02]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:07,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0061, 0.9939],\n",
      "        [0.2433, 0.7567],\n",
      "        [0.0153, 0.9847],\n",
      "        [0.0127, 0.9873]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        print(pred)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst.extend(pred[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056169d1-64a8-4b81-8daf-722b029cf2b9",
   "metadata": {},
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':file_lst, 'COVID':pred_lst})\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2c599e3-af43-4d60-bd76-9f593226d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7230e-01, 2.7700e-02],\n",
      "        [8.4935e-01, 1.5065e-01],\n",
      "        [8.1651e-01, 1.8349e-01],\n",
      "        [3.4291e-01, 6.5709e-01],\n",
      "        [9.8791e-01, 1.2090e-02],\n",
      "        [7.4627e-03, 9.9254e-01],\n",
      "        [6.6846e-01, 3.3154e-01],\n",
      "        [7.5879e-02, 9.2412e-01],\n",
      "        [9.8790e-01, 1.2102e-02],\n",
      "        [1.8778e-05, 9.9998e-01],\n",
      "        [9.7802e-01, 2.1982e-02],\n",
      "        [9.3440e-01, 6.5602e-02],\n",
      "        [9.8545e-01, 1.4554e-02],\n",
      "        [9.5419e-01, 4.5813e-02],\n",
      "        [9.0247e-01, 9.7534e-02],\n",
      "        [9.8905e-01, 1.0951e-02],\n",
      "        [4.4628e-01, 5.5372e-01],\n",
      "        [9.9488e-01, 5.1226e-03],\n",
      "        [4.7339e-01, 5.2661e-01],\n",
      "        [9.8920e-01, 1.0798e-02],\n",
      "        [9.9853e-01, 1.4723e-03],\n",
      "        [9.9292e-01, 7.0771e-03],\n",
      "        [9.2529e-01, 7.4712e-02],\n",
      "        [9.6305e-01, 3.6955e-02],\n",
      "        [9.6643e-01, 3.3571e-02],\n",
      "        [9.9677e-01, 3.2302e-03],\n",
      "        [9.9283e-01, 7.1667e-03],\n",
      "        [3.0243e-01, 6.9757e-01],\n",
      "        [4.9650e-03, 9.9504e-01],\n",
      "        [1.0714e-02, 9.8929e-01],\n",
      "        [3.0673e-02, 9.6933e-01],\n",
      "        [1.7424e-03, 9.9826e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1690, 0.8310],\n",
      "        [0.9627, 0.0373],\n",
      "        [0.9866, 0.0134],\n",
      "        [0.2451, 0.7549],\n",
      "        [0.6401, 0.3599],\n",
      "        [0.0494, 0.9506],\n",
      "        [0.0623, 0.9377],\n",
      "        [0.0279, 0.9721],\n",
      "        [0.0047, 0.9953],\n",
      "        [0.5879, 0.4121],\n",
      "        [0.9903, 0.0097],\n",
      "        [0.0054, 0.9946],\n",
      "        [0.9679, 0.0321],\n",
      "        [0.0144, 0.9856],\n",
      "        [0.0157, 0.9843],\n",
      "        [0.9964, 0.0036],\n",
      "        [0.9759, 0.0241],\n",
      "        [0.8684, 0.1316],\n",
      "        [0.0302, 0.9698],\n",
      "        [0.0030, 0.9970],\n",
      "        [0.2906, 0.7094],\n",
      "        [0.9656, 0.0344],\n",
      "        [0.0278, 0.9722],\n",
      "        [0.7928, 0.2072],\n",
      "        [0.5523, 0.4477],\n",
      "        [0.0060, 0.9940],\n",
      "        [0.9895, 0.0105],\n",
      "        [0.7135, 0.2865],\n",
      "        [0.0092, 0.9908],\n",
      "        [0.9496, 0.0504],\n",
      "        [0.9962, 0.0038],\n",
      "        [0.9905, 0.0095]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7271e-03, 9.9027e-01],\n",
      "        [4.7026e-04, 9.9953e-01],\n",
      "        [9.1508e-01, 8.4919e-02],\n",
      "        [9.6868e-01, 3.1316e-02],\n",
      "        [9.2225e-03, 9.9078e-01],\n",
      "        [9.8523e-01, 1.4766e-02],\n",
      "        [9.9175e-01, 8.2498e-03],\n",
      "        [9.9114e-01, 8.8600e-03],\n",
      "        [9.7607e-01, 2.3931e-02],\n",
      "        [9.9221e-01, 7.7887e-03],\n",
      "        [6.3900e-02, 9.3610e-01],\n",
      "        [9.9521e-01, 4.7896e-03],\n",
      "        [9.9834e-01, 1.6568e-03],\n",
      "        [9.9659e-01, 3.4119e-03],\n",
      "        [4.6501e-01, 5.3499e-01],\n",
      "        [3.6431e-03, 9.9636e-01],\n",
      "        [9.3479e-01, 6.5208e-02],\n",
      "        [9.2841e-01, 7.1593e-02],\n",
      "        [9.6488e-01, 3.5123e-02],\n",
      "        [1.7765e-02, 9.8223e-01],\n",
      "        [3.5839e-02, 9.6416e-01],\n",
      "        [8.6134e-01, 1.3866e-01],\n",
      "        [6.9404e-01, 3.0596e-01],\n",
      "        [9.1473e-01, 8.5270e-02],\n",
      "        [9.8796e-01, 1.2040e-02],\n",
      "        [1.1598e-01, 8.8402e-01],\n",
      "        [9.9233e-01, 7.6669e-03],\n",
      "        [9.8980e-01, 1.0197e-02],\n",
      "        [3.0205e-01, 6.9795e-01],\n",
      "        [1.2204e-02, 9.8780e-01],\n",
      "        [9.9740e-01, 2.6034e-03],\n",
      "        [9.6445e-01, 3.5550e-02]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:07,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0061, 0.9939],\n",
      "        [0.2433, 0.7567],\n",
      "        [0.0153, 0.9847],\n",
      "        [0.0127, 0.9873]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        print(pred)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.tolist())\n",
    "df = pd.DataFrame(pred_lst)\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('prediction111.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e5b27-f5eb-476a-b5f5-6022811ed1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
